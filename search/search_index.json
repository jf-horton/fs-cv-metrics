{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Foresight Metrics","text":"<p>Computer Vision Metrics Package - Extensible metrics for CV tasks.</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Foresight Metrics is a Python utility package for computing computer vision metrics with a focus on maintainability, readability, and extensibility.</p>"},{"location":"#supported-tasks","title":"Supported Tasks","text":"Task Metrics Object Detection mAP, mAP@0.5, mAP@0.75, Precision, Recall, F1 Segmentation mIoU, Dice, Pixel Accuracy"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf Simple API - One class per task, clear method signatures</li> <li>\ud83d\udd0c Extensible - Add custom formats and metrics without touching core code</li> <li>\ud83d\udcca Multiple Loggers - Console, ClearML, and custom integrations</li> <li>\ud83d\udcdd Self-Documenting - Full type hints and docstrings</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from foresight_metrics import ObjectDetection, StdoutLogger\n\nod = ObjectDetection(\n    data_format=\"coco\",\n    metrics=[\"mAP\", \"precision_recall\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = od.evaluate(\"ground_truth.json\", \"predictions.json\")\nprint(result.metrics)\n# {'mAP': 0.65, 'mAP@0.5': 0.82, 'mAP@0.75': 0.48, 'precision': 0.85, ...}\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -e .\n</code></pre> <p>With ClearML support: <pre><code>pip install -e \".[clearml]\"\n</code></pre></p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>API Reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed API documentation for all public classes and functions.</p>"},{"location":"api/#tasks","title":"Tasks","text":"<ul> <li>ObjectDetection - Object detection metrics (mAP, precision, recall)</li> <li>Segmentation - Segmentation metrics (mIoU, Dice, pixel accuracy)</li> </ul>"},{"location":"api/#loggers","title":"Loggers","text":"<ul> <li>StdoutLogger - Print metrics to terminal</li> <li>ClearMLLogger - Log metrics to ClearML</li> </ul>"},{"location":"api/#core","title":"Core","text":"<ul> <li>MetricResult - Metric computation result container</li> </ul>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>foresight_metrics/\n\u251c\u2500\u2500 __init__.py              # Public exports\n\u251c\u2500\u2500 results.py               # MetricResult\n\u251c\u2500\u2500 loggers/\n\u2502   \u251c\u2500\u2500 stdout.py            # StdoutLogger\n\u2502   \u2514\u2500\u2500 clearml.py           # ClearMLLogger\n\u2514\u2500\u2500 tasks/\n    \u251c\u2500\u2500 object_detection/\n    \u2502   \u251c\u2500\u2500 __init__.py      # ObjectDetection\n    \u2502   \u251c\u2500\u2500 types.py         # ODData\n    \u2502   \u251c\u2500\u2500 formats/         # CocoFormat, etc.\n    \u2502   \u2514\u2500\u2500 metrics/         # MAPMetric, PrecisionRecallMetric\n    \u2514\u2500\u2500 segmentation/\n        \u251c\u2500\u2500 __init__.py      # Segmentation\n        \u251c\u2500\u2500 types.py         # SegData\n        \u251c\u2500\u2500 formats/         # NumpyFormat\n        \u2514\u2500\u2500 metrics/         # IoUMetric, DiceMetric, PixelAccuracyMetric\n</code></pre>"},{"location":"api/loggers/","title":"Loggers","text":"<p>Loggers handle outputting metrics to various destinations.</p>"},{"location":"api/loggers/#stdoutlogger","title":"StdoutLogger","text":""},{"location":"api/loggers/#foresight_metrics.loggers.stdout.StdoutLogger","title":"<code>foresight_metrics.loggers.stdout.StdoutLogger</code>","text":"<p>Logger that prints metrics to standard output.</p> Example <p>from foresight_metrics.loggers import StdoutLogger logger = StdoutLogger() logger.log(result)  # Prints formatted metrics to terminal</p>"},{"location":"api/loggers/#foresight_metrics.loggers.stdout.StdoutLogger.log","title":"<code>log(result)</code>","text":"<p>Print the metric result to stdout.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>MetricResult</code> <p>The MetricResult to display.</p> required"},{"location":"api/loggers/#example","title":"Example","text":"<pre><code>from foresight_metrics import ObjectDetection, StdoutLogger\n\nod = ObjectDetection(loggers=[StdoutLogger()])\nresult = od.evaluate(\"gt.json\", \"preds.json\")\n\n# Output:\n# === object_detection ===\n#   mAP: 0.6500\n#   mAP@0.5: 0.8200\n#   mAP@0.75: 0.4800\n#   precision: 0.8500\n#   recall: 0.7200\n#   f1: 0.7800\n</code></pre>"},{"location":"api/loggers/#clearmllogger","title":"ClearMLLogger","text":""},{"location":"api/loggers/#foresight_metrics.loggers.clearml.ClearMLLogger","title":"<code>foresight_metrics.loggers.clearml.ClearMLLogger</code>","text":"<p>Logger that sends metrics to a ClearML task.</p> <p>This logger integrates with ClearML experiment tracking to log metrics as scalars that can be viewed in the ClearML web UI.</p> Example <p>from clearml import Task from foresight_metrics.loggers import ClearMLLogger</p> <p>task = Task.init(project_name=\"my_project\", task_name=\"eval\") logger = ClearMLLogger(task=task) logger.log(result)  # Metrics appear in ClearML dashboard</p>"},{"location":"api/loggers/#foresight_metrics.loggers.clearml.ClearMLLogger.__init__","title":"<code>__init__(task)</code>","text":"<p>Initialize the ClearML logger.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Any</code> <p>A ClearML Task object. The task should already be   initialized before passing it here.</p> required"},{"location":"api/loggers/#foresight_metrics.loggers.clearml.ClearMLLogger.log","title":"<code>log(result)</code>","text":"<p>Log metrics to the ClearML task.</p> <p>Logs each metric as a scalar with the task name as the title and metric name as the series.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>MetricResult</code> <p>The MetricResult containing metrics to log.</p> required"},{"location":"api/loggers/#example_1","title":"Example","text":"<pre><code>from clearml import Task\nfrom foresight_metrics import ObjectDetection, ClearMLLogger\n\ntask = Task.init(project_name=\"my_project\", task_name=\"evaluation\")\n\nod = ObjectDetection(loggers=[ClearMLLogger(task=task)])\nresult = od.evaluate(\"gt.json\", \"preds.json\")\n# Metrics appear in ClearML dashboard\n</code></pre>"},{"location":"api/loggers/#custom-loggers","title":"Custom Loggers","text":"<p>Implement the logger protocol to create custom loggers:</p> <pre><code>from foresight_metrics.results import MetricResult\n\nclass MyLogger:\n    def log(self, result: MetricResult) -&gt; None:\n        # Send metrics somewhere\n        pass\n</code></pre> <p>See Custom Extensions for more examples.</p>"},{"location":"api/object-detection/","title":"ObjectDetection","text":""},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection","title":"<code>foresight_metrics.tasks.object_detection.ObjectDetection</code>","text":"<p>Evaluate object detection models.</p> <p>This is the main entry point for computing object detection metrics. It handles loading data from various formats, computing metrics, and logging results to configured destinations.</p> <p>Attributes:</p> Name Type Description <code>data_format</code> <code>str</code> <p>Name of the data format being used.</p> <code>metric_names</code> <code>list[str]</code> <p>Names of metrics to compute.</p> <code>loggers</code> <code>list[str]</code> <p>List of loggers for output.</p> Example <p>from foresight_metrics.tasks.object_detection import ObjectDetection from foresight_metrics.loggers import StdoutLogger, ClearMLLogger</p> <p>od = ObjectDetection( ...     data_format=\"coco\", ...     metrics=[\"mAP\", \"precision_recall\"], ...     loggers=[StdoutLogger()], ... ) result = od.evaluate(\"ground_truth.json\", \"predictions.json\") print(result.metrics)</p> Extension Example"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection--add-a-custom-format","title":"Add a custom format","text":"<p>class YoloFormat: ...     name = \"yolo\" ...     def load(self, gt, preds): ...</p> <p>od = ObjectDetection( ...     data_format=\"yolo\", ...     custom_formats={\"yolo\": YoloFormat}, ... )</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection--add-a-custom-metric","title":"Add a custom metric","text":"<p>class MyMetric: ...     name = \"my_metric\" ...     def compute(self, data): return {\"my_metric\": 0.5}</p> <p>od = ObjectDetection( ...     metrics=[\"mAP\", \"my_metric\"], ...     custom_metrics={\"my_metric\": MyMetric}, ... )</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.data_format","title":"<code>data_format</code>  <code>property</code>","text":"<p>Name of the configured data format.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.metric_names","title":"<code>metric_names</code>  <code>property</code>","text":"<p>Names of metrics that will be computed.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.available_formats","title":"<code>available_formats</code>  <code>property</code>","text":"<p>All available format names (built-in + custom).</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.available_metrics","title":"<code>available_metrics</code>  <code>property</code>","text":"<p>All available metric names (built-in + custom).</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.__init__","title":"<code>__init__(data_format='coco', metrics=None, loggers=None, custom_formats=None, custom_metrics=None)</code>","text":"<p>Initialize the ObjectDetection evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>data_format</code> <code>str</code> <p>Name of the data format (\"coco\", \"yolo\", etc.).         Use custom_formats to add new formats.</p> <code>'coco'</code> <code>metrics</code> <code>Sequence[str] | None</code> <p>List of metric names to compute. Defaults to all     built-in metrics. Use custom_metrics to add new metrics.</p> <code>None</code> <code>loggers</code> <code>Sequence[MetricsLogger] | None</code> <p>List of loggers for output (StdoutLogger, ClearMLLogger, etc.).</p> <code>None</code> <code>custom_formats</code> <code>dict[str, Type[ODFormatAdapter]] | None</code> <p>Additional format adapters as {name: AdapterClass}.</p> <code>None</code> <code>custom_metrics</code> <code>dict[str, Type[ODMetric]] | None</code> <p>Additional metrics as {name: MetricClass}.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_format or metric name is not recognized.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.evaluate","title":"<code>evaluate(ground_truth, predictions)</code>","text":"<p>Evaluate predictions against ground truth.</p> <p>This method: 1. Loads data using the configured format adapter 2. Computes all requested metrics 3. Logs results to all configured loggers 4. Returns a MetricResult with all computed values</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <p>Path to GT file, or data structure in the          configured format.</p> required <code>predictions</code> <p>Path to predictions file, or data structure         in the configured format.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>MetricResult containing all computed metrics, with fields:</p> <code>MetricResult</code> <ul> <li>task_name: \"object_detection\"</li> </ul> <code>MetricResult</code> <ul> <li>metrics: dict of metric name -&gt; value</li> </ul> <code>MetricResult</code> <ul> <li>metadata: additional info about the evaluation</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data format is invalid.</p> <code>FileNotFoundError</code> <p>If file paths don't exist.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.ObjectDetection.evaluate_data","title":"<code>evaluate_data(data)</code>","text":"<p>Evaluate metrics directly from ODData.</p> <p>Use this when you've already constructed ODData in memory, e.g., from running inference in a loop. This bypasses the format adapter entirely.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ODData</code> <p>ODData containing ground truth and predictions.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>MetricResult containing all computed metrics.</p> Example <p>from foresight_metrics.tasks.object_detection.types import ODData import numpy as np</p> <p>data = ODData( ...     gt_image_ids=np.array([0, 0]), ...     gt_boxes=np.array([[10, 10, 50, 50], [60, 60, 100, 100]]), ...     gt_labels=np.array([0, 1]), ...     pred_image_ids=np.array([0, 0]), ...     pred_boxes=np.array([[12, 12, 48, 48], [58, 58, 102, 102]]), ...     pred_labels=np.array([0, 1]), ...     pred_scores=np.array([0.9, 0.85]), ... ) od = ObjectDetection() result = od.evaluate_data(data)</p>"},{"location":"api/object-detection/#oddata","title":"ODData","text":""},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData","title":"<code>foresight_metrics.tasks.object_detection.types.ODData</code>  <code>dataclass</code>","text":"<p>Internal representation of object detection data for metric computation.</p> <p>All metrics operate on this format. Format adapters convert external formats (COCO, YOLO, etc.) into this structure.</p> <p>Supports multi-image evaluation by including image_id arrays.</p> <p>Attributes:</p> Name Type Description <code>gt_image_ids</code> <code>ndarray</code> <p>Image ID for each ground truth box. Shape [N].</p> <code>gt_boxes</code> <code>ndarray</code> <p>Ground truth bounding boxes in xyxy format. Shape [N, 4].</p> <code>gt_labels</code> <code>ndarray</code> <p>Class labels for each ground truth box. Shape [N].</p> <code>pred_image_ids</code> <code>ndarray</code> <p>Image ID for each prediction. Shape [M].</p> <code>pred_boxes</code> <code>ndarray</code> <p>Predicted bounding boxes in xyxy format. Shape [M, 4].</p> <code>pred_labels</code> <code>ndarray</code> <p>Predicted class labels. Shape [M].</p> <code>pred_scores</code> <code>ndarray</code> <p>Confidence scores for predictions. Shape [M].</p> <code>class_names</code> <code>dict[int, str] | None</code> <p>Optional mapping from label ID to class name.</p> <code>image_paths</code> <code>dict[int, str] | None</code> <p>Optional mapping from image ID to file path.</p> Example <p>data = ODData( ...     gt_image_ids=np.array([0, 0, 1]), ...     gt_boxes=np.array([[10, 10, 50, 50], [60, 60, 100, 100], [20, 20, 80, 80]]), ...     gt_labels=np.array([0, 1, 0]), ...     pred_image_ids=np.array([0, 0, 1]), ...     pred_boxes=np.array([[12, 12, 48, 48], [58, 58, 102, 102], [22, 22, 78, 78]]), ...     pred_labels=np.array([0, 1, 0]), ...     pred_scores=np.array([0.9, 0.85, 0.75]), ... )</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData.num_gt_boxes","title":"<code>num_gt_boxes</code>  <code>property</code>","text":"<p>Total number of ground truth boxes.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData.num_predictions","title":"<code>num_predictions</code>  <code>property</code>","text":"<p>Total number of predictions.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData.num_images","title":"<code>num_images</code>  <code>property</code>","text":"<p>Number of unique images in the dataset.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData.unique_labels","title":"<code>unique_labels</code>  <code>property</code>","text":"<p>All unique class labels across GT and predictions.</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.types.ODData.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate data shapes after initialization.</p>"},{"location":"api/object-detection/#format-adapters","title":"Format Adapters","text":""},{"location":"api/object-detection/#cocoformat","title":"CocoFormat","text":""},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.formats.coco.CocoFormat","title":"<code>foresight_metrics.tasks.object_detection.formats.coco.CocoFormat</code>","text":"<p>Format adapter for COCO JSON annotation format.</p> <p>Supports loading ground truth and predictions from COCO-style JSON files. Uses the Supervision library internally for robust parsing when available, but falls back to direct JSON parsing.</p> <p>Expected ground truth format (COCO annotations):     {         \"images\": [{\"id\": 1, \"file_name\": \"image1.jpg\", ...}, ...],         \"annotations\": [{\"id\": 1, \"image_id\": 1, \"category_id\": 1, \"bbox\": [x, y, w, h], ...}, ...],         \"categories\": [{\"id\": 1, \"name\": \"person\"}, ...]     }</p> <p>Expected predictions format (COCO results):     [         {\"image_id\": 1, \"category_id\": 1, \"bbox\": [x, y, w, h], \"score\": 0.95},         ...     ]</p> Example <p>adapter = CocoFormat() data = adapter.load(\"annotations.json\", \"predictions.json\") print(data.num_gt_boxes, data.num_predictions)</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.formats.coco.CocoFormat.load","title":"<code>load(ground_truth, predictions)</code>","text":"<p>Load COCO format data and convert to ODData.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <code>Union[str, Path, dict]</code> <p>Path to COCO annotations JSON or dict with annotations.</p> required <code>predictions</code> <code>Union[str, Path, dict, list]</code> <p>Path to COCO results JSON, list of predictions, or dict.</p> required <p>Returns:</p> Type Description <code>ODData</code> <p>ODData with ground truth and predictions in canonical format.</p>"},{"location":"api/object-detection/#metrics","title":"Metrics","text":""},{"location":"api/object-detection/#mapmetric","title":"MAPMetric","text":""},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.map.MAPMetric","title":"<code>foresight_metrics.tasks.object_detection.metrics.map.MAPMetric</code>","text":"<p>Mean Average Precision (mAP) metric.</p> <p>Computes COCO-style mAP at multiple IoU thresholds. The default thresholds follow the COCO evaluation standard: [0.5, 0.55, ..., 0.95].</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>\"mAP\"</p> <code>iou_thresholds</code> <p>List of IoU thresholds for evaluation.</p> Example <p>metric = MAPMetric(iou_thresholds=[0.5, 0.75]) result = metric.compute(data) print(result)</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.map.MAPMetric.__init__","title":"<code>__init__(iou_thresholds=None)</code>","text":"<p>Initialize the mAP metric.</p> <p>Parameters:</p> Name Type Description Default <code>iou_thresholds</code> <code>Sequence[float] | None</code> <p>IoU thresholds for mAP calculation.            Defaults to COCO standard [0.5, 0.55, ..., 0.95].</p> <code>None</code>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.map.MAPMetric.compute","title":"<code>compute(data)</code>","text":"<p>Compute mAP across all classes and IoU thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ODData</code> <p>Object detection data in canonical format.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with mAP values:</p> <code>dict[str, float]</code> <ul> <li>\"mAP\": Mean AP across all thresholds</li> </ul> <code>dict[str, float]</code> <ul> <li>\"mAP@{threshold}\": AP at specific thresholds (0.5, 0.75)</li> </ul>"},{"location":"api/object-detection/#precisionrecallmetric","title":"PrecisionRecallMetric","text":""},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.precision_recall.PrecisionRecallMetric","title":"<code>foresight_metrics.tasks.object_detection.metrics.precision_recall.PrecisionRecallMetric</code>","text":"<p>Precision, Recall, and F1 score at a fixed IoU threshold.</p> <p>Computes detection metrics by matching predictions to ground truth boxes using IoU and class labels.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>\"precision_recall\"</p> <code>iou_threshold</code> <p>IoU threshold for counting a detection as correct.</p> <code>score_threshold</code> <p>Minimum confidence score for predictions.</p> Example <p>metric = PrecisionRecallMetric(iou_threshold=0.5) result = metric.compute(data) print(result)</p>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.precision_recall.PrecisionRecallMetric.__init__","title":"<code>__init__(iou_threshold=0.5, score_threshold=0.0)</code>","text":"<p>Initialize the precision/recall metric.</p> <p>Parameters:</p> Name Type Description Default <code>iou_threshold</code> <code>float</code> <p>IoU threshold for matching predictions to GT.</p> <code>0.5</code> <code>score_threshold</code> <code>float</code> <p>Minimum confidence score for predictions.</p> <code>0.0</code>"},{"location":"api/object-detection/#foresight_metrics.tasks.object_detection.metrics.precision_recall.PrecisionRecallMetric.compute","title":"<code>compute(data)</code>","text":"<p>Compute precision, recall, and F1 score.</p> <p>A prediction is considered a true positive if: 1. Its IoU with a GT box &gt;= iou_threshold 2. Its class matches the GT class 3. The GT box has not already been matched</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ODData</code> <p>Object detection data in canonical format.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with:</p> <code>dict[str, float]</code> <ul> <li>\"precision\": TP / (TP + FP)</li> </ul> <code>dict[str, float]</code> <ul> <li>\"recall\": TP / (TP + FN)</li> </ul> <code>dict[str, float]</code> <ul> <li>\"f1\": Harmonic mean of precision and recall</li> </ul>"},{"location":"api/results/","title":"MetricResult","text":""},{"location":"api/results/#foresight_metrics.results.MetricResult","title":"<code>foresight_metrics.results.MetricResult</code>  <code>dataclass</code>","text":"<p>Container for metric computation results.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>Name of the vision task (e.g., \"object_detection\")</p> <code>metrics</code> <code>dict[str, float]</code> <p>Dictionary mapping metric names to values</p> <code>per_class</code> <code>dict[str, dict[str, float]]</code> <p>Optional per-class breakdown of metrics</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Optional additional information about the evaluation</p> Example <p>result = MetricResult( ...     task_name=\"object_detection\", ...     metrics={\"mAP\": 0.75, \"precision\": 0.82}, ...     per_class={\"person\": {\"precision\": 0.85}, \"car\": {\"precision\": 0.78}}, ... ) print(result.pretty())</p>"},{"location":"api/results/#foresight_metrics.results.MetricResult.pretty","title":"<code>pretty()</code>","text":"<p>Format results for terminal display.</p> <p>Returns:</p> Type Description <code>str</code> <p>Human-readable string representation of metrics.</p>"},{"location":"api/results/#foresight_metrics.results.MetricResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Export as plain dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation suitable for JSON serialization.</p>"},{"location":"api/results/#example-usage","title":"Example Usage","text":"<pre><code>from foresight_metrics import ObjectDetection\n\nod = ObjectDetection()\nresult = od.evaluate(\"gt.json\", \"preds.json\")\n\n# Access computed metrics\nprint(result.metrics)\n# {'mAP': 0.65, 'mAP@0.5': 0.82, 'mAP@0.75': 0.48, 'precision': 0.85, ...}\n\n# Task name\nprint(result.task_name)\n# 'object_detection'\n\n# Metadata about the evaluation\nprint(result.metadata)\n# {'format': 'coco', 'num_gt_boxes': 100, 'num_predictions': 95, ...}\n\n# Pretty print for terminal\nprint(result.pretty())\n# === object_detection ===\n#   mAP: 0.6500\n#   mAP@0.5: 0.8200\n#   ...\n\n# Export to dictionary\ndata = result.to_dict()\n</code></pre>"},{"location":"api/results/#attributes","title":"Attributes","text":"Attribute Type Description <code>task_name</code> <code>str</code> Name of the evaluation task <code>metrics</code> <code>dict[str, float]</code> Computed metric values <code>per_class</code> <code>dict[str, dict[str, float]]</code> Per-class metric breakdown <code>metadata</code> <code>dict</code> Additional evaluation information"},{"location":"api/results/#methods","title":"Methods","text":"Method Returns Description <code>pretty()</code> <code>str</code> Formatted string for terminal display <code>to_dict()</code> <code>dict</code> Dictionary for JSON serialization"},{"location":"api/segmentation/","title":"Segmentation","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation","title":"<code>foresight_metrics.tasks.segmentation.Segmentation</code>","text":"<p>Evaluate semantic segmentation models.</p> <p>This is the main entry point for computing segmentation metrics. It handles loading data from various formats, computing metrics, and logging results to configured destinations.</p> <p>Attributes:</p> Name Type Description <code>data_format</code> <code>str</code> <p>Name of the data format being used.</p> <code>metric_names</code> <code>list[str]</code> <p>Names of metrics to compute.</p> <code>loggers</code> <code>list[str]</code> <p>List of loggers for output.</p> Example <p>from foresight_metrics.tasks.segmentation import Segmentation from foresight_metrics.loggers import StdoutLogger import numpy as np</p> <p>seg = Segmentation( ...     data_format=\"numpy\", ...     num_classes=3, ...     metrics=[\"iou\", \"dice\"], ...     loggers=[StdoutLogger()], ... ) gt = np.array([[[0, 0, 1], [1, 1, 2], [2, 2, 2]]]) pred = np.array([[[0, 0, 1], [1, 1, 1], [2, 2, 2]]]) result = seg.evaluate(gt, pred) print(result.metrics)</p> Extension Example"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation--add-a-custom-metric","title":"Add a custom metric","text":"<p>class MyMetric: ...     name = \"my_metric\" ...     def compute(self, data): return {\"my_metric\": 0.5}</p> <p>seg = Segmentation( ...     num_classes=3, ...     metrics=[\"iou\", \"my_metric\"], ...     custom_metrics={\"my_metric\": MyMetric}, ... )</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.data_format","title":"<code>data_format</code>  <code>property</code>","text":"<p>Name of the configured data format.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.num_classes","title":"<code>num_classes</code>  <code>property</code>","text":"<p>Number of classes in the dataset.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.metric_names","title":"<code>metric_names</code>  <code>property</code>","text":"<p>Names of metrics that will be computed.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.available_formats","title":"<code>available_formats</code>  <code>property</code>","text":"<p>All available format names (built-in + custom).</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.available_metrics","title":"<code>available_metrics</code>  <code>property</code>","text":"<p>All available metric names (built-in + custom).</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.__init__","title":"<code>__init__(num_classes, data_format='numpy', metrics=None, loggers=None, custom_formats=None, custom_metrics=None, ignore_index=255)</code>","text":"<p>Initialize the Segmentation evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset (required).</p> required <code>data_format</code> <code>str</code> <p>Name of the data format (\"numpy\", etc.).         Use custom_formats to add new formats.</p> <code>'numpy'</code> <code>metrics</code> <code>Sequence[str] | None</code> <p>List of metric names to compute. Defaults to all     built-in metrics. Use custom_metrics to add new metrics.</p> <code>None</code> <code>loggers</code> <code>Sequence[MetricsLogger] | None</code> <p>List of loggers for output (StdoutLogger, ClearMLLogger, etc.).</p> <code>None</code> <code>custom_formats</code> <code>dict[str, Type[SegFormatAdapter]] | None</code> <p>Additional format adapters as {name: AdapterClass}.</p> <code>None</code> <code>custom_metrics</code> <code>dict[str, Type[SegMetric]] | None</code> <p>Additional metrics as {name: MetricClass}.</p> <code>None</code> <code>ignore_index</code> <code>int</code> <p>Class index to ignore in metric computation.</p> <code>255</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_format or metric name is not recognized.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.Segmentation.evaluate","title":"<code>evaluate(ground_truth, predictions)</code>","text":"<p>Evaluate predictions against ground truth.</p> <p>This method: 1. Loads data using the configured format adapter 2. Computes all requested metrics 3. Logs results to all configured loggers 4. Returns a MetricResult with all computed values</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <p>Path to GT file(s), or numpy array of masks.</p> required <code>predictions</code> <p>Path to prediction file(s), or numpy array of masks.</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>MetricResult containing all computed metrics, with fields:</p> <code>MetricResult</code> <ul> <li>task_name: \"segmentation\"</li> </ul> <code>MetricResult</code> <ul> <li>metrics: dict of metric name -&gt; value</li> </ul> <code>MetricResult</code> <ul> <li>metadata: additional info about the evaluation</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data format is invalid.</p> <code>FileNotFoundError</code> <p>If file paths don't exist.</p>"},{"location":"api/segmentation/#segdata","title":"SegData","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.types.SegData","title":"<code>foresight_metrics.tasks.segmentation.types.SegData</code>  <code>dataclass</code>","text":"<p>Internal representation of segmentation data for metric computation.</p> <p>All metrics operate on this format. Format adapters convert external formats (COCO, PNG masks, etc.) into this structure.</p> <p>Supports both semantic and instance segmentation: - For semantic segmentation: use gt_masks and pred_masks as 2D label maps - For instance segmentation: use per-instance masks with labels</p> <p>Attributes:</p> Name Type Description <code>gt_masks</code> <code>ndarray</code> <p>Ground truth segmentation masks. Shape [N, H, W] for N images,       where each pixel contains the class label.</p> <code>pred_masks</code> <code>ndarray</code> <p>Predicted segmentation masks. Shape [N, H, W].</p> <code>class_names</code> <code>dict[int, str] | None</code> <p>Optional mapping from class ID to class name.</p> <code>ignore_index</code> <code>int</code> <p>Class index to ignore in metric computation (default: 255).</p> <code>num_classes</code> <code>int</code> <p>Number of classes (excluding ignore index).</p> <p>Example (semantic segmentation):     &gt;&gt;&gt; data = SegData(     ...     gt_masks=np.array([[[0, 0, 1], [1, 1, 2], [2, 2, 2]]]),  # [1, 3, 3]     ...     pred_masks=np.array([[[0, 0, 1], [1, 1, 1], [2, 2, 2]]]),     ...     num_classes=3,     ... )</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.types.SegData.num_images","title":"<code>num_images</code>  <code>property</code>","text":"<p>Number of images in the dataset.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.types.SegData.image_shape","title":"<code>image_shape</code>  <code>property</code>","text":"<p>Height and width of the masks.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.types.SegData.total_pixels","title":"<code>total_pixels</code>  <code>property</code>","text":"<p>Total number of pixels across all images.</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.types.SegData.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate data shapes after initialization.</p>"},{"location":"api/segmentation/#format-adapters","title":"Format Adapters","text":""},{"location":"api/segmentation/#numpyformat","title":"NumpyFormat","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.formats.numpy_format.NumpyFormat","title":"<code>foresight_metrics.tasks.segmentation.formats.numpy_format.NumpyFormat</code>","text":"<p>Format adapter for numpy arrays.</p> <p>Accepts ground truth and predictions as numpy arrays directly. This is the simplest format adapter and is useful when masks are already loaded in memory.</p> Expected format <ul> <li>Ground truth: np.ndarray of shape [N, H, W] or [H, W]</li> <li>Predictions: np.ndarray of shape [N, H, W] or [H, W]</li> <li>Values are class labels (integers)</li> </ul> Example <p>adapter = NumpyFormat() gt = np.array([[[0, 0, 1], [1, 1, 2], [2, 2, 2]]]) pred = np.array([[[0, 0, 1], [1, 1, 1], [2, 2, 2]]]) data = adapter.load(gt, pred, num_classes=3)</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.formats.numpy_format.NumpyFormat.load","title":"<code>load(ground_truth, predictions, num_classes)</code>","text":"<p>Load numpy arrays and convert to SegData.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth</code> <code>Union[ndarray, str, Path]</code> <p>Numpy array of shape [N, H, W] or [H, W], or path to .npy file.</p> required <code>predictions</code> <code>Union[ndarray, str, Path]</code> <p>Numpy array of shape [N, H, W] or [H, W], or path to .npy file.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset.</p> required <p>Returns:</p> Type Description <code>SegData</code> <p>SegData with masks in canonical format.</p>"},{"location":"api/segmentation/#metrics","title":"Metrics","text":""},{"location":"api/segmentation/#ioumetric","title":"IoUMetric","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.iou.IoUMetric","title":"<code>foresight_metrics.tasks.segmentation.metrics.iou.IoUMetric</code>","text":"<p>Intersection over Union (IoU) metric for semantic segmentation.</p> <p>Computes per-class IoU and mean IoU (mIoU) across all classes. IoU is defined as: intersection / union = TP / (TP + FP + FN)</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>\"iou\"</p> Example <p>metric = IoUMetric() result = metric.compute(data) print(result)</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.iou.IoUMetric.compute","title":"<code>compute(data)</code>","text":"<p>Compute IoU metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SegData</code> <p>Segmentation data in canonical format.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with:</p> <code>dict[str, float]</code> <ul> <li>\"mIoU\": Mean IoU across all classes</li> </ul> <code>dict[str, float]</code> <ul> <li>\"IoU_class_{i}\": Per-class IoU (if class_names not provided)</li> </ul> <code>dict[str, float]</code> <ul> <li>\"IoU_{name}\": Per-class IoU (if class_names provided)</li> </ul>"},{"location":"api/segmentation/#dicemetric","title":"DiceMetric","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.dice.DiceMetric","title":"<code>foresight_metrics.tasks.segmentation.metrics.dice.DiceMetric</code>","text":"<p>Dice coefficient (F1 score) for semantic segmentation.</p> <p>Computes per-class Dice score and mean Dice across all classes. Dice is defined as: 2 * intersection / (|A| + |B|) = 2TP / (2TP + FP + FN)</p> <p>This is equivalent to the F1 score for binary classification.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>\"dice\"</p> Example <p>metric = DiceMetric() result = metric.compute(data) print(result)</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.dice.DiceMetric.compute","title":"<code>compute(data)</code>","text":"<p>Compute Dice coefficient metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SegData</code> <p>Segmentation data in canonical format.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with:</p> <code>dict[str, float]</code> <ul> <li>\"mean_dice\": Mean Dice across all classes</li> </ul> <code>dict[str, float]</code> <ul> <li>\"dice_class_{i}\": Per-class Dice (if class_names not provided)</li> </ul> <code>dict[str, float]</code> <ul> <li>\"dice_{name}\": Per-class Dice (if class_names provided)</li> </ul>"},{"location":"api/segmentation/#pixelaccuracymetric","title":"PixelAccuracyMetric","text":""},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.dice.PixelAccuracyMetric","title":"<code>foresight_metrics.tasks.segmentation.metrics.dice.PixelAccuracyMetric</code>","text":"<p>Pixel accuracy for semantic segmentation.</p> <p>Computes overall pixel accuracy and per-class accuracy.</p> <p>Attributes:</p> Name Type Description <code>name</code> <p>\"pixel_accuracy\"</p> Example <p>metric = PixelAccuracyMetric() result = metric.compute(data) print(result)</p>"},{"location":"api/segmentation/#foresight_metrics.tasks.segmentation.metrics.dice.PixelAccuracyMetric.compute","title":"<code>compute(data)</code>","text":"<p>Compute pixel accuracy metrics.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>SegData</code> <p>Segmentation data in canonical format.</p> required <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>Dictionary with:</p> <code>dict[str, float]</code> <ul> <li>\"pixel_accuracy\": Overall pixel accuracy</li> </ul> <code>dict[str, float]</code> <ul> <li>\"mean_class_accuracy\": Mean per-class accuracy</li> </ul>"},{"location":"contributing/documentation/","title":"Maintaining Documentation","text":"<p>This guide explains how to update and maintain the Foresight Metrics documentation.</p>"},{"location":"contributing/documentation/#overview","title":"Overview","text":"<p>The documentation uses MkDocs with:</p> <ul> <li>Material theme \u2014 Modern, responsive design</li> <li>mkdocstrings \u2014 Auto-generates API docs from Python docstrings</li> </ul>"},{"location":"contributing/documentation/#quick-start","title":"Quick Start","text":"<pre><code># Install docs dependencies\npip install -e \".[docs]\"\n\n# Serve locally with auto-reload\nmkdocs serve\n\n# Open http://127.0.0.1:8000 in your browser\n</code></pre>"},{"location":"contributing/documentation/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                    # Homepage\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 installation.md         # Installation guide\n\u2502   \u2514\u2500\u2500 quickstart.md           # Quick start tutorial\n\u251c\u2500\u2500 user-guide/\n\u2502   \u251c\u2500\u2500 object-detection.md     # OD user guide\n\u2502   \u251c\u2500\u2500 segmentation.md         # Segmentation user guide\n\u2502   \u2514\u2500\u2500 extensions.md           # How to extend\n\u251c\u2500\u2500 api/                        # API Reference (auto-generated)\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 object-detection.md\n\u2502   \u251c\u2500\u2500 segmentation.md\n\u2502   \u251c\u2500\u2500 loggers.md\n\u2502   \u2514\u2500\u2500 results.md\n\u2514\u2500\u2500 contributing/\n    \u2514\u2500\u2500 documentation.md        # This file\n</code></pre>"},{"location":"contributing/documentation/#what-auto-updates-vs-manual-updates","title":"What Auto-Updates vs Manual Updates","text":"Content Auto-updates? Location Class/method signatures \u2705 Yes Docstrings in Python code Parameter descriptions \u2705 Yes Docstrings in Python code Examples in docstrings \u2705 Yes Docstrings in Python code User guide examples \u274c No <code>docs/user-guide/*.md</code> Metric/format tables \u274c No <code>docs/user-guide/*.md</code> Installation steps \u274c No <code>docs/getting-started/installation.md</code>"},{"location":"contributing/documentation/#updating-api-documentation","title":"Updating API Documentation","text":"<p>API docs are generated from Python docstrings using mkdocstrings.</p>"},{"location":"contributing/documentation/#example-updating-a-class-description","title":"Example: Updating a Class Description","text":"<p>Edit the docstring in Python:</p> <pre><code># foresight_metrics/tasks/object_detection/__init__.py\n\nclass ObjectDetection:\n    \"\"\"\n    Evaluate object detection models.  # \u2190 This becomes the description\n\n    This class handles loading data, computing metrics, and logging.\n\n    Attributes:\n        data_format: Name of the data format being used.\n\n    Example:\n        &gt;&gt;&gt; od = ObjectDetection()\n        &gt;&gt;&gt; result = od.evaluate(\"gt.json\", \"preds.json\")\n    \"\"\"\n</code></pre> <p>The API reference page will automatically reflect these changes on next build.</p>"},{"location":"contributing/documentation/#adding-a-new-class-to-api-docs","title":"Adding a New Class to API Docs","text":"<ol> <li>Create or edit the relevant <code>.md</code> file in <code>docs/api/</code></li> <li>Add the mkdocstrings directive:</li> </ol> <pre><code>::: foresight_metrics.path.to.YourClass\n    options:\n      show_root_heading: true\n      show_source: false\n</code></pre>"},{"location":"contributing/documentation/#updating-user-guides","title":"Updating User Guides","text":"<p>User guides in <code>docs/user-guide/</code> are hand-written. Update them when:</p> <ul> <li>API signatures change</li> <li>New features are added</li> <li>Examples become outdated</li> </ul>"},{"location":"contributing/documentation/#checklist-for-new-features","title":"Checklist for New Features","text":"<p>When adding a new metric, format, or task:</p> <ul> <li>[ ] Update \"Supported Metrics/Formats\" table in relevant user guide</li> <li>[ ] Add example usage if needed</li> <li>[ ] Update <code>docs/api/</code> if new classes are exposed</li> <li>[ ] Update <code>docs/index.md</code> if it's a major feature</li> </ul>"},{"location":"contributing/documentation/#writing-good-docstrings","title":"Writing Good Docstrings","text":"<p>Follow Google-style docstrings for consistency:</p> <pre><code>def evaluate(self, ground_truth, predictions) -&gt; MetricResult:\n    \"\"\"\n    Evaluate predictions against ground truth.\n\n    This method loads data, computes metrics, and logs results.\n\n    Args:\n        ground_truth: Path to GT file or data structure.\n        predictions: Path to predictions file or data structure.\n\n    Returns:\n        MetricResult containing all computed metrics.\n\n    Raises:\n        ValueError: If data format is invalid.\n        FileNotFoundError: If file paths don't exist.\n\n    Example:\n        &gt;&gt;&gt; od = ObjectDetection()\n        &gt;&gt;&gt; result = od.evaluate(\"gt.json\", \"preds.json\")\n        &gt;&gt;&gt; print(result.metrics)\n    \"\"\"\n</code></pre>"},{"location":"contributing/documentation/#building-and-deploying","title":"Building and Deploying","text":""},{"location":"contributing/documentation/#local-development","title":"Local Development","text":"<pre><code>mkdocs serve  # Auto-reloads on changes\n</code></pre>"},{"location":"contributing/documentation/#build-static-site","title":"Build Static Site","text":"<pre><code>mkdocs build  # Output in site/\n</code></pre>"},{"location":"contributing/documentation/#deploy-to-github-pages","title":"Deploy to GitHub Pages","text":"<pre><code>mkdocs gh-deploy  # Pushes to gh-pages branch\n</code></pre> <p>This will make docs available at: https://jf-horton.github.io/fs-cv-metrics/</p>"},{"location":"contributing/documentation/#configuration","title":"Configuration","text":"<p>The MkDocs configuration is in <code>mkdocs.yml</code>:</p> <pre><code># Key settings\nsite_name: Foresight Metrics\ntheme:\n  name: material\n\nplugins:\n  - mkdocstrings  # Auto-generate from docstrings\n\nnav:\n  - Home: index.md\n  - ...  # Navigation structure\n</code></pre>"},{"location":"contributing/documentation/#tips","title":"Tips","text":"<ol> <li>Keep examples in docstrings \u2014 They auto-update and stay in sync</li> <li>Use <code>mkdocs serve</code> \u2014 See changes instantly while editing</li> <li>Check links \u2014 Broken links will show warnings during build</li> <li>Preview before deploying \u2014 Run <code>mkdocs build</code> and check <code>site/</code></li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>NumPy</li> </ul>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install from the repository:</p> <pre><code>git clone https://github.com/jf-horton/fs-cv-metrics.git\ncd fs-cv-metrics\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#clearml-integration","title":"ClearML Integration","text":"<p>For logging metrics to ClearML experiment tracker:</p> <pre><code>pip install -e \".[clearml]\"\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For development (testing, linting, type checking):</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#all-dependencies","title":"All Dependencies","text":"<p>Install everything:</p> <pre><code>pip install -e \".[all]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from foresight_metrics import ObjectDetection, Segmentation\n\nprint(\"ObjectDetection available formats:\", ObjectDetection().available_formats)\nprint(\"ObjectDetection available metrics:\", ObjectDetection().available_metrics)\n</code></pre> <p>Expected output: <pre><code>ObjectDetection available formats: ['coco']\nObjectDetection available metrics: ['mAP', 'precision_recall']\n</code></pre></p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide shows you how to evaluate your first model with Foresight Metrics.</p>"},{"location":"getting-started/quickstart/#object-detection","title":"Object Detection","text":""},{"location":"getting-started/quickstart/#from-coco-json-files","title":"From COCO JSON Files","text":"<pre><code>from foresight_metrics import ObjectDetection, StdoutLogger\n\nod = ObjectDetection(\n    data_format=\"coco\",\n    metrics=[\"mAP\", \"precision_recall\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = od.evaluate(\"ground_truth.json\", \"predictions.json\")\n</code></pre>"},{"location":"getting-started/quickstart/#from-in-memory-data-inference-loop","title":"From In-Memory Data (Inference Loop)","text":"<p>When running inference in a loop, accumulate detections and evaluate at the end:</p> <pre><code>import numpy as np\nfrom foresight_metrics import ObjectDetection, StdoutLogger\nfrom foresight_metrics.tasks.object_detection.types import ODData\n\n# Accumulate during inference\nall_gt_image_ids = []\nall_gt_boxes = []\nall_gt_labels = []\nall_pred_image_ids = []\nall_pred_boxes = []\nall_pred_labels = []\nall_pred_scores = []\n\nfor idx, (image, annotation) in enumerate(dataset):\n    preds = model.predict(image)\n\n    # Accumulate ground truth\n    all_gt_image_ids.extend([idx] * len(annotation.boxes))\n    all_gt_boxes.append(annotation.boxes)\n    all_gt_labels.extend(annotation.labels)\n\n    # Accumulate predictions\n    all_pred_image_ids.extend([idx] * len(preds.boxes))\n    all_pred_boxes.append(preds.boxes)\n    all_pred_labels.extend(preds.labels)\n    all_pred_scores.extend(preds.scores)\n\n# Build ODData\ndata = ODData(\n    gt_image_ids=np.array(all_gt_image_ids),\n    gt_boxes=np.vstack(all_gt_boxes),\n    gt_labels=np.array(all_gt_labels),\n    pred_image_ids=np.array(all_pred_image_ids),\n    pred_boxes=np.vstack(all_pred_boxes),\n    pred_labels=np.array(all_pred_labels),\n    pred_scores=np.array(all_pred_scores),\n)\n\n# Evaluate\nod = ObjectDetection(loggers=[StdoutLogger()])\nresult = od.evaluate_data(data)\n</code></pre>"},{"location":"getting-started/quickstart/#segmentation","title":"Segmentation","text":"<pre><code>import numpy as np\nfrom foresight_metrics import Segmentation, StdoutLogger\n\n# Create sample masks (3 classes, 4x4 images)\ngt_masks = np.array([\n    [[0, 0, 1, 1],\n     [0, 0, 1, 1],\n     [2, 2, 2, 2],\n     [2, 2, 2, 2]],\n])\n\npred_masks = np.array([\n    [[0, 0, 1, 1],\n     [0, 1, 1, 1],  # one error\n     [2, 2, 2, 2],\n     [2, 2, 2, 2]],\n])\n\nseg = Segmentation(\n    num_classes=3,\n    data_format=\"numpy\",\n    metrics=[\"iou\", \"dice\", \"pixel_accuracy\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = seg.evaluate(gt_masks, pred_masks)\n</code></pre>"},{"location":"getting-started/quickstart/#with-clearml","title":"With ClearML","text":"<pre><code>from clearml import Task\nfrom foresight_metrics import ObjectDetection, ClearMLLogger, StdoutLogger\n\ntask = Task.init(project_name=\"my_project\", task_name=\"evaluation\")\n\nod = ObjectDetection(\n    data_format=\"coco\",\n    loggers=[\n        StdoutLogger(),\n        ClearMLLogger(task=task),\n    ],\n)\n\nresult = od.evaluate(\"gt.json\", \"preds.json\")\n# Metrics are logged to ClearML dashboard automatically\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-results","title":"Working with Results","text":"<p>The <code>evaluate()</code> method returns a <code>MetricResult</code> object:</p> <pre><code>result = od.evaluate(\"gt.json\", \"preds.json\")\n\n# Access metrics\nprint(result.metrics)        # {'mAP': 0.65, 'precision': 0.85, ...}\nprint(result.task_name)      # 'object_detection'\nprint(result.metadata)       # {'format': 'coco', 'num_gt_boxes': 100, ...}\n\n# Pretty print\nprint(result.pretty())\n\n# Export to dict\ndata = result.to_dict()\n</code></pre>"},{"location":"user-guide/extensions/","title":"Custom Extensions","text":"<p>Foresight Metrics is designed to be easily extensible. You can add custom formats, metrics, and loggers without modifying the core package.</p>"},{"location":"user-guide/extensions/#adding-a-custom-format","title":"Adding a Custom Format","text":"<p>Create a class that implements the format protocol:</p> <pre><code>from foresight_metrics.tasks.object_detection.types import ODData\nimport numpy as np\n\nclass YoloFormat:\n    \"\"\"Custom format adapter for YOLO text files.\"\"\"\n    name = \"yolo\"\n\n    def load(self, ground_truth, predictions) -&gt; ODData:\n        # Parse your format and return ODData\n        # ground_truth and predictions can be file paths or data structures\n\n        return ODData(\n            gt_image_ids=np.array([...]),\n            gt_boxes=np.array([...]),\n            gt_labels=np.array([...]),\n            pred_image_ids=np.array([...]),\n            pred_boxes=np.array([...]),\n            pred_labels=np.array([...]),\n            pred_scores=np.array([...]),\n        )\n</code></pre> <p>Use your custom format:</p> <pre><code>from foresight_metrics import ObjectDetection\n\nod = ObjectDetection(\n    data_format=\"yolo\",\n    custom_formats={\"yolo\": YoloFormat},\n)\n\nresult = od.evaluate(\"labels/\", \"predictions/\")\n</code></pre>"},{"location":"user-guide/extensions/#adding-a-custom-metric","title":"Adding a Custom Metric","text":"<p>Create a class that implements the metric protocol:</p> <pre><code>from foresight_metrics.tasks.object_detection.types import ODData\n\nclass ConfusionMatrixMetric:\n    \"\"\"Custom metric that computes confusion matrix statistics.\"\"\"\n    name = \"confusion\"\n\n    def __init__(self, iou_threshold: float = 0.5):\n        self.iou_threshold = iou_threshold\n\n    def compute(self, data: ODData) -&gt; dict[str, float]:\n        # Access data fields:\n        # - data.gt_boxes, data.gt_labels, data.gt_image_ids\n        # - data.pred_boxes, data.pred_labels, data.pred_scores, data.pred_image_ids\n\n        # Compute your metrics\n        tp = ...\n        fp = ...\n        fn = ...\n\n        return {\n            \"true_positives\": float(tp),\n            \"false_positives\": float(fp),\n            \"false_negatives\": float(fn),\n        }\n</code></pre> <p>Use your custom metric:</p> <pre><code>from foresight_metrics import ObjectDetection\n\nod = ObjectDetection(\n    metrics=[\"mAP\", \"confusion\"],\n    custom_metrics={\"confusion\": ConfusionMatrixMetric},\n)\n\nresult = od.evaluate(\"gt.json\", \"preds.json\")\nprint(result.metrics[\"true_positives\"])\n</code></pre>"},{"location":"user-guide/extensions/#adding-a-custom-logger","title":"Adding a Custom Logger","text":"<p>Create a class that implements the logger protocol:</p> <pre><code>from foresight_metrics.results import MetricResult\nimport json\n\nclass JSONFileLogger:\n    \"\"\"Logger that saves metrics to a JSON file.\"\"\"\n\n    def __init__(self, output_path: str):\n        self.output_path = output_path\n\n    def log(self, result: MetricResult) -&gt; None:\n        with open(self.output_path, \"w\") as f:\n            json.dump(result.to_dict(), f, indent=2)\n</code></pre> <p>Use your custom logger:</p> <pre><code>from foresight_metrics import ObjectDetection\n\nod = ObjectDetection(\n    loggers=[JSONFileLogger(\"metrics.json\")],\n)\n\nresult = od.evaluate(\"gt.json\", \"preds.json\")\n# Metrics saved to metrics.json\n</code></pre>"},{"location":"user-guide/extensions/#adding-a-new-task","title":"Adding a New Task","text":"<p>To add a completely new task type (e.g., keypoint detection), create a new directory under <code>tasks/</code>:</p> <pre><code>foresight_metrics/tasks/keypoints/\n\u251c\u2500\u2500 __init__.py      # KeypointDetection class\n\u251c\u2500\u2500 types.py         # KPData internal format\n\u251c\u2500\u2500 formats/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py      # KPFormatAdapter protocol\n\u2502   \u2514\u2500\u2500 coco.py      # COCO keypoint format\n\u2514\u2500\u2500 metrics/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 base.py      # KPMetric protocol\n    \u2514\u2500\u2500 oks.py       # Object Keypoint Similarity\n</code></pre> <p>Follow the same patterns used in <code>object_detection/</code> and <code>segmentation/</code>.</p>"},{"location":"user-guide/extensions/#protocol-reference","title":"Protocol Reference","text":""},{"location":"user-guide/extensions/#format-adapter-protocol","title":"Format Adapter Protocol","text":"<pre><code>class FormatAdapter:\n    name: str\n\n    def load(self, ground_truth, predictions) -&gt; InternalData:\n        ...\n</code></pre>"},{"location":"user-guide/extensions/#metric-protocol","title":"Metric Protocol","text":"<pre><code>class Metric:\n    name: str\n\n    def compute(self, data: InternalData) -&gt; dict[str, float]:\n        ...\n</code></pre>"},{"location":"user-guide/extensions/#logger-protocol","title":"Logger Protocol","text":"<pre><code>class Logger:\n    def log(self, result: MetricResult) -&gt; None:\n        ...\n</code></pre>"},{"location":"user-guide/object-detection/","title":"Object Detection","text":"<p>The <code>ObjectDetection</code> class provides metrics for evaluating object detection models.</p>"},{"location":"user-guide/object-detection/#supported-metrics","title":"Supported Metrics","text":"Metric Key Description mAP <code>mAP</code> Mean Average Precision across IoU thresholds 0.5-0.95 mAP@0.5 <code>mAP@0.5</code> AP at IoU threshold 0.5 mAP@0.75 <code>mAP@0.75</code> AP at IoU threshold 0.75 Precision <code>precision</code> True positives / (True positives + False positives) Recall <code>recall</code> True positives / (True positives + False negatives) F1 <code>f1</code> Harmonic mean of precision and recall"},{"location":"user-guide/object-detection/#supported-formats","title":"Supported Formats","text":"Format Key Description COCO <code>coco</code> Standard COCO JSON annotation format"},{"location":"user-guide/object-detection/#usage","title":"Usage","text":""},{"location":"user-guide/object-detection/#basic-usage","title":"Basic Usage","text":"<pre><code>from foresight_metrics import ObjectDetection, StdoutLogger\n\nod = ObjectDetection(\n    data_format=\"coco\",\n    metrics=[\"mAP\", \"precision_recall\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = od.evaluate(\"ground_truth.json\", \"predictions.json\")\n</code></pre>"},{"location":"user-guide/object-detection/#direct-data-evaluation","title":"Direct Data Evaluation","text":"<p>Use <code>evaluate_data()</code> when you have detection data in memory:</p> <pre><code>from foresight_metrics.tasks.object_detection.types import ODData\nimport numpy as np\n\ndata = ODData(\n    gt_image_ids=np.array([0, 0, 1]),\n    gt_boxes=np.array([[10, 10, 50, 50], [60, 60, 100, 100], [20, 20, 80, 80]]),\n    gt_labels=np.array([0, 1, 0]),\n    pred_image_ids=np.array([0, 0, 1]),\n    pred_boxes=np.array([[12, 12, 48, 48], [58, 58, 102, 102], [22, 22, 78, 78]]),\n    pred_labels=np.array([0, 1, 0]),\n    pred_scores=np.array([0.9, 0.85, 0.75]),\n)\n\nod = ObjectDetection()\nresult = od.evaluate_data(data)\n</code></pre>"},{"location":"user-guide/object-detection/#oddata-format","title":"ODData Format","text":"<p>The internal <code>ODData</code> format represents detection data:</p> Field Type Description <code>gt_image_ids</code> <code>np.ndarray[N]</code> Image ID for each ground truth box <code>gt_boxes</code> <code>np.ndarray[N, 4]</code> GT boxes in <code>[x1, y1, x2, y2]</code> format <code>gt_labels</code> <code>np.ndarray[N]</code> Class labels for GT (integers) <code>pred_image_ids</code> <code>np.ndarray[M]</code> Image ID for each prediction <code>pred_boxes</code> <code>np.ndarray[M, 4]</code> Predicted boxes in <code>[x1, y1, x2, y2]</code> <code>pred_labels</code> <code>np.ndarray[M]</code> Predicted class labels <code>pred_scores</code> <code>np.ndarray[M]</code> Confidence scores <code>class_names</code> <code>dict[int, str]</code> Optional label \u2192 name mapping"},{"location":"user-guide/segmentation/","title":"Segmentation","text":"<p>The <code>Segmentation</code> class provides metrics for evaluating semantic segmentation models.</p>"},{"location":"user-guide/segmentation/#supported-metrics","title":"Supported Metrics","text":"Metric Key Description mIoU <code>mIoU</code> Mean Intersection over Union across all classes IoU per class <code>IoU_class_{i}</code> Per-class IoU values Mean Dice <code>mean_dice</code> Mean Dice coefficient (F1 score) Dice per class <code>dice_class_{i}</code> Per-class Dice values Pixel Accuracy <code>pixel_accuracy</code> Overall pixel classification accuracy Mean Class Accuracy <code>mean_class_accuracy</code> Average per-class accuracy"},{"location":"user-guide/segmentation/#supported-formats","title":"Supported Formats","text":"Format Key Description NumPy <code>numpy</code> Direct numpy array input"},{"location":"user-guide/segmentation/#usage","title":"Usage","text":""},{"location":"user-guide/segmentation/#basic-usage","title":"Basic Usage","text":"<pre><code>import numpy as np\nfrom foresight_metrics import Segmentation, StdoutLogger\n\ngt_masks = np.array([...])    # Shape: [N, H, W] or [H, W]\npred_masks = np.array([...])  # Shape: [N, H, W] or [H, W]\n\nseg = Segmentation(\n    num_classes=3,\n    data_format=\"numpy\",\n    metrics=[\"iou\", \"dice\", \"pixel_accuracy\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = seg.evaluate(gt_masks, pred_masks)\n</code></pre>"},{"location":"user-guide/segmentation/#complete-example","title":"Complete Example","text":"<pre><code>import numpy as np\nfrom foresight_metrics import Segmentation, StdoutLogger\n\n# Ground truth: 2 images, 4x4 pixels, 3 classes\ngt_masks = np.array([\n    [[0, 0, 1, 1],\n     [0, 0, 1, 1],\n     [2, 2, 2, 2],\n     [2, 2, 2, 2]],\n    [[0, 0, 0, 0],\n     [1, 1, 1, 1],\n     [2, 2, 2, 2],\n     [2, 2, 2, 2]],\n])\n\n# Predictions (with some errors)\npred_masks = np.array([\n    [[0, 0, 1, 1],\n     [0, 1, 1, 1],  # one error\n     [2, 2, 2, 2],\n     [2, 2, 2, 2]],\n    [[0, 0, 0, 0],\n     [1, 1, 1, 1],\n     [2, 2, 1, 2],  # one error\n     [2, 2, 2, 2]],\n])\n\nseg = Segmentation(\n    num_classes=3,\n    metrics=[\"iou\", \"dice\"],\n    loggers=[StdoutLogger()],\n)\n\nresult = seg.evaluate(gt_masks, pred_masks)\n# Output:\n# mIoU: 0.8708\n# mean_dice: 0.9300\n</code></pre>"},{"location":"user-guide/segmentation/#segdata-format","title":"SegData Format","text":"<p>The internal <code>SegData</code> format represents segmentation data:</p> Field Type Description <code>gt_masks</code> <code>np.ndarray[N, H, W]</code> Ground truth masks (pixel values = class IDs) <code>pred_masks</code> <code>np.ndarray[N, H, W]</code> Predicted masks <code>num_classes</code> <code>int</code> Number of classes in the dataset <code>ignore_index</code> <code>int</code> Class index to ignore (default: 255) <code>class_names</code> <code>dict[int, str]</code> Optional label \u2192 name mapping"},{"location":"user-guide/segmentation/#ignoring-classes","title":"Ignoring Classes","text":"<p>Use <code>ignore_index</code> to exclude certain pixels (e.g., unlabeled regions):</p> <pre><code>seg = Segmentation(\n    num_classes=3,\n    ignore_index=255,  # Pixels with value 255 are ignored\n)\n</code></pre>"}]}